# VLA (Vision-Language-Action) ëª¨ë¸ ë„ì… ê³„íš

## ë°œê²¬í•œ ê²½ëŸ‰ VLA ëª¨ë¸ë“¤

### 1. SmolVLA (450M params) - ê°€ì¥ ìœ ë§!
- **ì¶œì²˜**: [HuggingFace Blog](https://huggingface.co/blog/smolvla) | [arXiv](https://arxiv.org/abs/2506.01844)
- **í¬ê¸°**: 450M íŒŒë¼ë¯¸í„° (7B ëŒ€ë¹„ 15ë°° ì‘ìŒ)
- **ì„±ëŠ¥**: 10-20ë°° í° ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥
- **íŠ¹ì§•**:
  - ì‹±ê¸€ GPU í•™ìŠµ ê°€ëŠ¥
  - Consumer GPU/CPUì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
  - ë¹„ë™ê¸° ì¶”ë¡ ìœ¼ë¡œ 30% ë¹ ë¥¸ ì‘ë‹µ
  - ì˜¤í”ˆì†ŒìŠ¤ (LeRobot ë°ì´í„°ì…‹)

```
SmolVLA ì•„í‚¤í…ì²˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SmolVLM2 (Vision-Language Model)   â”‚
â”‚  â”œâ”€â”€ SigLIP (Vision Encoder)        â”‚
â”‚  â””â”€â”€ SmolLM2 (Language Decoder)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Action Expert (MLP)           â”‚
â”‚       â†’ Robot Control Output        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. NaVILA (Navigation VLA with LiDAR)
- **ì¶œì²˜**: [Project Page](https://navila-bot.github.io/) | [GitHub](https://github.com/AnjieCheng/NaVILA)
- **íŠ¹ì§•**: Vision + LiDAR ì‚¬ìš©!
- **êµ¬ì¡°**: 2-ë ˆë²¨ ì‹œìŠ¤í…œ
  - High-level: VLA â†’ ì¤‘ê°„ ë ˆë²¨ ëª…ë ¹ ("75cm ì „ì§„")
  - Low-level: LiDAR ê¸°ë°˜ locomotion policy
- **ì„±ëŠ¥**: ì‹¤ì œ í™˜ê²½ì—ì„œ 88% ì„±ê³µë¥ 

```
NaVILA ì•„í‚¤í…ì²˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera     â”‚â”€â”€â”€â–¶â”‚   VLA Model      â”‚
â”‚   Image      â”‚    â”‚ (High-level)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ "move forward 75cm"
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LiDAR      â”‚â”€â”€â”€â–¶â”‚   Locomotion     â”‚
â”‚   2.5D Map   â”‚    â”‚   RL Policy      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                      Robot Actions
```

---

## Vision + LiDAR VLA: í˜„ì¬ ìƒíƒœ

### ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹
| ëª¨ë¸ | Vision | LiDAR | í†µí•© ë°©ì‹ |
|------|--------|-------|-----------|
| OpenVLA | âœ… | âŒ | Vision only |
| SmolVLA | âœ… | âŒ | Vision only |
| NaVILA | âœ… | âœ… | ë¶„ë¦¬ (2-level) |
| MLA (2025) | âœ… | âœ… | Multisensory fusion |

### ê²°ë¡ : ì§ì ‘ Vision+LiDAR VLAëŠ” ê±°ì˜ ì—†ìŒ!
- ëŒ€ë¶€ë¶„ Visionë§Œ ì‚¬ìš©
- LiDARëŠ” ë³„ë„ ì•ˆì „ ì‹œìŠ¤í…œìœ¼ë¡œ ì‚¬ìš©
- í†µí•©ëœ End-to-end ëª¨ë¸ì€ ì—°êµ¬ ì´ˆê¸° ë‹¨ê³„

---

## ğŸš€ Vision+LiDAR VLA ê°œë°œ ê³„íš

### ëª©í‘œ
```
ì…ë ¥: Camera Image + LiDAR Scan + Language Command
ì¶œë ¥: Robot Action (linear_vel, angular_vel)
```

### ì œì•ˆ ì•„í‚¤í…ì²˜: "LidarVLA"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LidarVLA Model                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Camera Image â”‚  â”‚ LiDAR Scan   â”‚  â”‚ Language Cmd â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                 â”‚          â”‚
â”‚         â–¼                 â–¼                 â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SigLIP       â”‚  â”‚ PointNet     â”‚  â”‚ SmolLM2      â”‚  â”‚
â”‚  â”‚ Encoder      â”‚  â”‚ or BEV CNN   â”‚  â”‚ Tokenizer    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                 â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                  â”‚                 â”‚                   â”‚
â”‚                  â–¼                 â–¼                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚    Cross-Modal Attention     â”‚               â”‚
â”‚         â”‚    (Vision-LiDAR-Language)   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                        â”‚                               â”‚
â”‚                        â–¼                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚     Action Expert (MLP)      â”‚               â”‚
â”‚         â”‚  â†’ (linear_vel, angular_vel) â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 1: ë°ì´í„° ìˆ˜ì§‘ (2ì£¼)

```python
# ìˆ˜ì§‘í•  ë°ì´í„° êµ¬ì¡°
{
    "camera_image": "640x480 RGB",
    "lidar_scan": "360 points",
    "language_cmd": "go to the red ball",
    "action": {
        "linear_vel": 0.2,
        "angular_vel": 0.1
    },
    "success": True
}
```

**ìˆ˜ì§‘ ë°©ë²•:**
1. ìˆ˜ë™ ì¡°ì‘í•˜ë©° ë°ì´í„° ê¸°ë¡
2. ê° ì—í”¼ì†Œë“œì— ìì—°ì–´ ëª…ë ¹ ë¼ë²¨ë§
3. ëª©í‘œ: 1000+ ì—í”¼ì†Œë“œ

**ROS2 ë°ì´í„° ìˆ˜ì§‘ ë…¸ë“œ:**
```python
# data_collector.py
class DataCollector(Node):
    def __init__(self):
        # ë™ê¸°í™”ëœ êµ¬ë…
        self.camera_sub = Subscriber(Image, '/camera/image_raw/compressed')
        self.lidar_sub = Subscriber(LaserScan, '/scan')
        self.cmd_sub = Subscriber(Twist, '/cmd_vel')

        # ë™ê¸°í™”
        self.sync = ApproximateTimeSynchronizer(
            [self.camera_sub, self.lidar_sub, self.cmd_sub],
            queue_size=10, slop=0.1
        )
```

### Phase 2: LiDAR ì¸ì½”ë” ê°œë°œ (1ì£¼)

**Option A: BEV (Bird's Eye View) CNN**
```python
class LidarBEVEncoder(nn.Module):
    def __init__(self):
        # LiDAR â†’ 2D ì´ë¯¸ì§€ (ìœ„ì—ì„œ ë³¸ ì‹œì )
        self.bev_size = (64, 64)
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8))
        )

    def lidar_to_bev(self, scan):
        # 360ë„ ìŠ¤ìº” â†’ 64x64 occupancy grid
        ...
```

**Option B: PointNet (ì§ì ‘ í¬ì¸íŠ¸ ì²˜ë¦¬)**
```python
class LidarPointEncoder(nn.Module):
    def __init__(self):
        self.mlp1 = nn.Linear(2, 64)   # (x, y) per point
        self.mlp2 = nn.Linear(64, 128)
        self.mlp3 = nn.Linear(128, 256)

    def forward(self, points):
        # [B, N, 2] â†’ [B, 256]
        x = F.relu(self.mlp1(points))
        x = F.relu(self.mlp2(x))
        x = self.mlp3(x)
        return x.max(dim=1)[0]  # Max pooling
```

### Phase 3: SmolVLA ìˆ˜ì • (2ì£¼)

```python
# ê¸°ì¡´ SmolVLA í™•ì¥
class LidarVLA(SmolVLA):
    def __init__(self):
        super().__init__()

        # LiDAR ì¸ì½”ë” ì¶”ê°€
        self.lidar_encoder = LidarBEVEncoder()

        # Cross-modal fusion
        self.fusion = nn.MultiheadAttention(
            embed_dim=512,
            num_heads=8
        )

    def forward(self, image, lidar, text):
        # Vision features
        vis_feat = self.vision_encoder(image)

        # LiDAR features
        lid_feat = self.lidar_encoder(lidar)

        # Language features
        lang_feat = self.language_encoder(text)

        # Fusion
        fused = self.fusion(
            query=lang_feat,
            key=torch.cat([vis_feat, lid_feat], dim=1),
            value=torch.cat([vis_feat, lid_feat], dim=1)
        )

        # Action prediction
        action = self.action_head(fused)
        return action
```

### Phase 4: í•™ìŠµ (1ì£¼)

```python
# í•™ìŠµ ì„¤ì •
config = {
    "model": "LidarVLA-small",
    "batch_size": 16,
    "lr": 1e-4,
    "epochs": 50,
    "device": "cuda",  # Jetson GPU

    # ë°ì´í„° ì¦ê°•
    "augment": {
        "image": ["flip", "color_jitter"],
        "lidar": ["rotate", "noise"],
        "text": ["paraphrase"]
    }
}
```

### Phase 5: ìµœì í™” & ë°°í¬ (1ì£¼)

```bash
# TensorRT ë³€í™˜
python export_tensorrt.py \
    --model lidar_vla.pt \
    --precision fp16 \
    --output lidar_vla.engine

# ROS2 ë…¸ë“œë¡œ ë°°í¬
ros2 run robot_ai lidar_vla_node.py
```

---

## êµ¬í˜„ ë¡œë“œë§µ

```
Week 1-2: ë°ì´í„° ìˆ˜ì§‘
â”œâ”€â”€ ROS2 ë°ì´í„° ìˆ˜ì§‘ ë…¸ë“œ ê°œë°œ
â”œâ”€â”€ ìˆ˜ë™ ì¡°ì‘ìœ¼ë¡œ ì—í”¼ì†Œë“œ ê¸°ë¡
â””â”€â”€ ìì—°ì–´ ëª…ë ¹ ë¼ë²¨ë§

Week 3: LiDAR ì¸ì½”ë”
â”œâ”€â”€ BEV ë³€í™˜ êµ¬í˜„
â”œâ”€â”€ CNN ì¸ì½”ë” í•™ìŠµ
â””â”€â”€ ë‹¨ë… ì„±ëŠ¥ ê²€ì¦

Week 4-5: ëª¨ë¸ í†µí•©
â”œâ”€â”€ SmolVLA ì½”ë“œ í¬í¬
â”œâ”€â”€ LiDAR ë¸Œëœì¹˜ ì¶”ê°€
â”œâ”€â”€ Cross-modal attention êµ¬í˜„
â””â”€â”€ í†µí•© í•™ìŠµ

Week 6: ìµœì í™” & ë°°í¬
â”œâ”€â”€ TensorRT ë³€í™˜
â”œâ”€â”€ ROS2 ë…¸ë“œ êµ¬í˜„
â””â”€â”€ ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸
```

---

## ì˜ˆìƒ ì„±ëŠ¥

| ë©”íŠ¸ë¦­ | ëª©í‘œ |
|--------|------|
| ëª¨ë¸ í¬ê¸° | ~500M params |
| ì¶”ë¡  ì†ë„ | 10+ Hz (Jetson) |
| ë©”ëª¨ë¦¬ | < 4GB VRAM |
| ì„±ê³µë¥  | > 70% (ê°„ë‹¨í•œ ëª…ë ¹) |

---

## í•„ìš” ë¦¬ì†ŒìŠ¤

- **í•˜ë“œì›¨ì–´**: Jetson Orin Nano (í˜„ì¬ ë³´ìœ )
- **ë°ì´í„°**: ìì²´ ìˆ˜ì§‘ (1000+ ì—í”¼ì†Œë“œ)
- **ë² ì´ìŠ¤ ëª¨ë¸**: SmolVLA (ì˜¤í”ˆì†ŒìŠ¤)
- **í”„ë ˆì„ì›Œí¬**: PyTorch + TensorRT

---

## ì°¸ê³  ìë£Œ

- [SmolVLA - HuggingFace](https://huggingface.co/blog/smolvla)
- [NaVILA - GitHub](https://github.com/AnjieCheng/NaVILA)
- [LeRobot - HuggingFace](https://github.com/huggingface/lerobot)
- [VLA Survey](https://vla-survey.github.io/)

---

## ê²°ë¡ 

**Vision+LiDAR VLAëŠ” ì•„ì§ ì—†ë‹¤!** â†’ ë§Œë“¤ ê°€ì¹˜ ìˆìŒ

**SmolVLA ê¸°ë°˜ìœ¼ë¡œ:**
1. LiDAR ì¸ì½”ë” ì¶”ê°€
2. Cross-modal fusion êµ¬í˜„
3. ìì²´ ë°ì´í„°ë¡œ fine-tune
4. Jetson ìµœì í™”

**ì˜ˆìƒ ê°œë°œ ê¸°ê°„: 6ì£¼**
